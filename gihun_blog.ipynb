{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "urls = []\n",
    "titles = []\n",
    "dates = []\n",
    "authors = []\n",
    "\n",
    "def get_urls():\n",
    "    for i in range(9501,10001):\n",
    "        try:\n",
    "            res = requests.get(f'http://blog.cnyes.com/SearchArticle.aspx?KindId=23&PageIndex={i}')\n",
    "            soup = BeautifulSoup(res.text,'html.parser')\n",
    "            tr_s = soup.select_one('table.list01')\n",
    "            tr_s = tr_s.select('tr')\n",
    "#            print(len(tr_s))\n",
    "            for tr in tr_s:\n",
    "                titles.append(tr.select_one('td:nth-child(1) > a:nth-child(4)').text)\n",
    "                dates.append(tr.select_one('span.date').text)\n",
    "                authors.append(tr.select_one('span.keywordem').text)\n",
    "                urls.append(tr.select_one('td:nth-child(1) > a:nth-child(4)').get('href'))\n",
    "            \n",
    "#            break\n",
    "        except:\n",
    "            print('Error')\n",
    "            print(f'http://blog.cnyes.com/SearchArticle.aspx?KindId=23&PageIndex={i}')\n",
    "            continue\n",
    "\n",
    "#url_list = []\n",
    "#title_list = []\n",
    "#author_list = []\n",
    "#date_list = []\n",
    "content_list = []\n",
    "#count = 0\n",
    "\n",
    "# requests.get需設置timeout 5秒未連線產生except\n",
    "def get_content():\n",
    "    count = 0\n",
    "    for url in urls:\n",
    "        try:\n",
    "            res = requests.get('http://blog.cnyes.com'+url, timeout=5)\n",
    "            soup = BeautifulSoup(res.text,'html.parser')\n",
    "            content_list.append(soup.select_one('div.article').text)\n",
    "\n",
    "        except:\n",
    "            print(url)\n",
    "            content_list.append('')\n",
    "        count +=1\n",
    "        print('處理了:',(count/len(urls))*100,'%')\n",
    "\n",
    "def save_csv():            \n",
    "#    pd_store= pd.DataFrame(columns = ['URL','Date','Title','Author','Content'])\n",
    "    pd_store= pd.DataFrame(columns = ['URL','Date','Title','Author'])\n",
    "    pd_store['URL'] = urls\n",
    "    pd_store['Date'] = dates\n",
    "    pd_store['Title'] = titles\n",
    "    pd_store['Author'] = authors\n",
    "#    pd_store['Content'] = content_list\n",
    "\n",
    "#    pd_store.to_csv('gihun_blog_7.csv', index=False)\n",
    "    pd_store.to_csv('gihun_blog_13_urls.csv', index=False)\n",
    "\n",
    "# 先抓各個網頁的網址    \n",
    "# main\n",
    "get_urls()\n",
    "#get_content()\n",
    "save_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "f = pd.read_csv('gihunblog_url/gihun_blog_7_2_urls.csv')\n",
    "\n",
    "content_list = []\n",
    "# requests.get需設置timeout 5秒未連線產生except 否則很有可能卡在一個網頁很久\n",
    "def get_content():\n",
    "    urls = df['URL']\n",
    "    count = 0\n",
    "    for url in urls:\n",
    "        try:\n",
    "            res = requests.get('http://blog.cnyes.com'+url, timeout=5)\n",
    "#            print(res)\n",
    "            soup = BeautifulSoup(res.text,'html.parser')\n",
    "#            print(soup)\n",
    "            content_list.append(soup.select_one('div.article').text)\n",
    "\n",
    "        except:\n",
    "            print('http://blog.cnyes.com'+url)\n",
    "            content_list.append('')\n",
    "        count +=1\n",
    "        print('處理了:',(count/len(urls))*100,'%')\n",
    "#        if count > 1:\n",
    "#            break\n",
    "def save_csv():\n",
    "    df['content'] = content_list\n",
    "\n",
    "    df.to_csv('gihun_blog_7_2.csv', index=False)\n",
    "\n",
    "# main\n",
    "get_content()\n",
    "save_csv()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
