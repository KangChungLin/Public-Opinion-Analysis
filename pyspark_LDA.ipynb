{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .master(\"yarn\")\\\n",
    "        .config('spark.executor.instances','99')\\\n",
    "        .config('spark.executor.memory','4G')\\\n",
    "        .appName(\"mldemo\")\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read csv\n",
    "sales = spark.read.format(\"csv\")\\\n",
    "  .option(\"header\", \"true\")\\\n",
    "  .option(\"inferSchema\", \"true\")\\\n",
    "  .load(\"/user/spark/share/words_all.csv\")\\\n",
    "  .where(\"Words IS NOT NULL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+-------------------------------------+--------------------+---------------+--------------------------------+\n",
      "|                 URL|      Date|                                Title|              Author|        Website|                           Words|\n",
      "+--------------------+----------+-------------------------------------+--------------------+---------------+--------------------------------+\n",
      "|https://wealth.bu...|2020/07/20|       國安基金不退場，股市會過熱嗎？|              陳唯泰|bussinessweekly| 意外 國安 基金 不 退場 話說 ...|\n",
      "|https://wealth.bu...|2020/07/20|台積電去美國不夠，還要去日本？讀賣...|黃嘉洵編譯／經濟日報|bussinessweekly|  讀賣 新聞 19日 報導 日本 政...|\n",
      "|https://wealth.bu...|2020/07/20|  華為衝擊已過＋5G需求加持，外資看...|            Atkinson|bussinessweekly| 亞系 外資 在 最 新 研究 報告...|\n",
      "|https://wealth.bu...|2020/07/17| 台股萬點以上就是貴？從「1指標」看...|     Charlotte夏綠蒂|bussinessweekly|    2020年 3月 股災 全球 股市...|\n",
      "|https://wealth.bu...|2020/07/17|金融海嘯大賠出場，老婆抱著他一起哭...|                孫太|bussinessweekly|夫妻 因為 投資 理念 不同 產生...|\n",
      "+--------------------+----------+-------------------------------------+--------------------+---------------+--------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 計算詞頻\n",
    "from pyspark.ml.feature import Tokenizer, CountVectorizer\n",
    "tkn = Tokenizer().setInputCol(\"Words\").setOutputCol(\"TokenOut\")\n",
    "tokenized = tkn.transform(sales.select(\"Words\"))\n",
    "cv = CountVectorizer()\\\n",
    "  .setInputCol(\"TokenOut\")\\\n",
    "  .setOutputCol(\"features\")\\\n",
    "  .setVocabSize(500)\\\n",
    "  .setMinTF(0)\\\n",
    "  .setMinDF(0)\\\n",
    "  .setBinary(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvFitted = cv.fit(tokenized)\n",
    "prepped = cvFitted.transform(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+----------------------------+--------------------+\n",
      "|                           Words|                    TokenOut|            features|\n",
      "+--------------------------------+----------------------------+--------------------+\n",
      "| 意外 國安 基金 不 退場 話說 ...|[意外, 國安, 基金, 不, 退...|(500,[0,1,2,3,5,6...|\n",
      "|  讀賣 新聞 19日 報導 日本 政...| [讀賣, 新聞, 19日, 報導,...|(500,[0,2,5,6,18,...|\n",
      "| 亞系 外資 在 最 新 研究 報告...| [亞系, 外資, 在, 最, 新,...|(500,[0,1,2,3,4,5...|\n",
      "|    2020年 3月 股災 全球 股市...|   [2020年, 3月, 股災, 全...|(500,[0,1,2,3,6,7...|\n",
      "|夫妻 因為 投資 理念 不同 產生...|[夫妻, 因為, 投資, 理念, ...|(500,[0,1,2,3,4,5...|\n",
      "+--------------------------------+----------------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prepped.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.99 ms, sys: 14 ms, total: 17.9 ms\n",
      "Wall time: 36.7 s\n"
     ]
    }
   ],
   "source": [
    "%time cvFitted = cv.fit(tokenized)\n",
    "prepped = cvFitted.transform(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 將詞頻用LDA進行文件分類\n",
    "# setK() 分幾類\n",
    "from pyspark.ml.clustering import LDA\n",
    "lda = LDA().setK(5).setMaxIter(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lda.fit(prepped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.25 ms, sys: 9.68 ms, total: 14.9 ms\n",
      "Wall time: 25.7 s\n"
     ]
    }
   ],
   "source": [
    "%time model = lda.fit(prepped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+--------------------+\n",
      "|topic|termIndices|         termWeights|\n",
      "+-----+-----------+--------------------+\n",
      "|    0| [0, 2, 10]|[0.01191710462391...|\n",
      "|    1|  [0, 1, 3]|[0.01995731890938...|\n",
      "|    2|[0, 4, 119]|[0.01587687698025...|\n",
      "|    3| [0, 14, 2]|[0.01194581290519...|\n",
      "|    4|  [0, 3, 2]|[0.00963768413344...|\n",
      "+-----+-----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.describeTopics(3).show()\n",
    "#cvFitted.vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 計算tf-idf\n",
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "tf = HashingTF()\\\n",
    "  .setInputCol(\"DescOut\")\\\n",
    "  .setOutputCol(\"TFOut\")\\\n",
    "  .setNumFeatures(10000)\n",
    "idf = IDF()\\\n",
    "  .setInputCol(\"TFOut\")\\\n",
    "  .setOutputCol(\"IDFOut\")\\\n",
    "  .setMinDocFreq(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf.fit(tf.transform(tfIdfIn)).transform(tf.transform(tfIdfIn)).show(10, False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
